{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments with Mistral7B-Instruct (baseline) and fine tuned model with Aventures texts**\n",
    "\n",
    "The idea of the experiments consists in using the Classifier trained for authorship attribution (BertAA_Wilde_vs_Mistral) to test the capacity of the fine tuned model to mimetize Wilde's writting style.\n",
    "\n",
    "Then, for each model we are going to generate fiction stories from the same starting lines and assign labels to each story and compare the results between the two models. The hypothesis is that for the fine tuned model the capacity of the model to distinguish between Wilde and Mistral will decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 10:46:25.263291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 10:46:25.263323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 10:46:25.264217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 10:46:25.268240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 10:46:25.934465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import the required packages\n",
    "import torch\n",
    "#import simpletransformers\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, platform, gradio, warnings\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and models paths\n",
    "dir_root = './AVENTURES' # comment this line to run in google colab\n",
    "dir_data = f'{dir_root}/data'\n",
    "models_path = f'{dir_root}/models'\n",
    "list_to_generate_path = f'{dir_data}/story_prompts_for_evaluation.txt'\n",
    "ft_model = f'{dir_root}/models/Mistral7B_fine_tuned_AVENTURES.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lines to start the generation\n",
    "texts_to_generation = []\n",
    "with open(list_to_generate_path, 'r+', encoding='utf-8') as fd:\n",
    "  texts_to_generation = fd.readlines()\n",
    "texts_to_generation = [text[:-1] for text in texts_to_generation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments with baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8c763881de4c0b99f615293ecf71f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model(model_name)\n",
    "tokenizer = load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_preds, generated_texts = clf_exp(model, tokenizer, clf, texts_to_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predictions:\\n Wilde: {author_preds.count(0)/len(author_preds) * 100} % \\n Mistral7B: {author_preds.count(1)/len(author_preds) * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver the results\n",
    "save_generated_texts(generated_texts, author_preds, model = 'baseline', data_path = dir_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments with fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae93e88a68846f39f647d92f2df04ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model(model_name, adapt = True, from_finetuned = True, model_path = ft_model)\n",
    "tokenizer = load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_generation = [\"Enfants, à vos pièces, dit Kernok en se précipitant sur son banc de quart le porte-voix à la main ; à vos pièces, et, sacredieu ! ne faites pas feu avant le commandement. L'abordage !... l'abordage !… On se suspend au cordage, On s'élance des haubans,\",\n",
    "                       \"Que mon frère écoute son ami, la langue du chef ne sera pas fourchue, il ne dira que ce qui est vrai. — Je vous écoute, chef. Je sais que vos paroles seront celles d’un sachem parlant avec un ami. — Ce matin, un peu avant le lever du soleil, je traversais avec mes guerriers la Vallée des ombres, mes guerriers suivaient l’orée de la forêt,\",\n",
    "                       \"Il brandille à Montfaucon, au bout d’une chaîne de fer, en attendant que sa carcasse déchiquetée des oiseaux tombe en la fosse du gibet, sur les ossements des camarades qui l’ont précédé. — C’est donc cela, dit Lampourde avec le plus beau sang-froid du monde, qu’on ne le voyait pas depuis quelque temps.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_exp(model, tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Function for the experiments: Given a model, a tokenizer and a classifier, it generates text using the \n",
    "    lines in the texts list and then predicts the label of the generated text using the classifier\n",
    "    inputs:\n",
    "      model: model: model to use\n",
    "      tokenizer: tokenizer: tokenizer to use\n",
    "      texts: list: list of strings with the lines to generate and predict\n",
    "    outputs:\n",
    "        label_predictions: list: list of predictions of the classifier\n",
    "        generated_texts: list: list of the generated texts\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # pattern to remove the prompt tokens after generation\n",
    "    patt = r'\\[INST]|\\[\\/INST]|\\<s>|\\</s>|This are the first lines of a work of fiction. Continue it.'\n",
    "\n",
    "    generated_texts = [] # list of generated texts\n",
    "    label_predictions = [] # list of predictions of the classifier\n",
    "    for input in tqdm(texts):\n",
    "      tokens = tokenize(tokenizer, input)\n",
    "      model_inputs = tokens.to(device)\n",
    "      generated_ids = model.generate(**model_inputs, max_new_tokens=500, do_sample=True)\n",
    "      decoded = tokenizer.batch_decode(generated_ids)\n",
    "      decoded = [re.sub(patt, '', x) for x in decoded] # clean the generated texts\n",
    "      print(decoded)\n",
    "      generated_texts.extend(decoded)\n",
    "      del model_inputs\n",
    "      del decoded\n",
    "      del generated_ids\n",
    "    return label_predictions, generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|██████████████████████████████████████████████████████████▋                                                                                                                     | 1/3 [00:20<00:40, 20.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"  Enfants, à vos pièces, dit Kernok en se précipitant sur son banc de quart le porte-voix à la main ; à vos pièces, et, sacredieu ! ne faites pas feu avant le commandement. L'abordage !... l'abordage !… On se suspend au cordage, On s'élance des haubans,  à chaque commandement, et il s'élance le dernier pour la rallier… et, à quelle heure, s'amuse-t-on les enfants ? À vos pièces, enfants… On fait feu… on fait feu… on fait feu… ;  on fait feu… On fait feu… on fait feu… on fait feu… on fait feu… on fait feu…  À chaque commandement, la petite Catherine se précipitait des haubans, et sans faire feu que le commandement pour l'abordage lui eût fait dire à Kernok. Le capitaine était devenue bien mécontente. Son attente n'a pas été du tout exagérée, mais si ses enfants pouvaient s'amuser au sort du capitaine ennemi, il ne fallait pas dire d'autres choses que le capitaine Kernok ? Si on pouvaient se contenter de l'amusement des siens, il ne fallait pas dire aux autres de la petite Cathy qu'un capitaine était capturé ? Le capitaine Kernok était un brave qui avait reçu une blessure au combat, un médecin l'avait ravi dans son lit de mort ; si Kernok pouvait se tirer de cette réchute, et, du coup de choc, pouvait se retourner à son coup de choc ; il n'aurait pas besoin de s'amuser, il avait d'autres choses à faire qu'il deviendrait fâché  ; il pouvait se retourner aux objets de sa réchute d'homme, de guerriers, d'insurgents, qu'il possédait jusqu'à sa blessure. Mais quand le médecin l'avait arraché aux pieds de ses maîtres en lui demandant que le docteur fît par égards à celui qui se retrouvait là le grand patron du navire qui, lorsqu'il était dans les eaux du Nord, portait un certain nom qui lui était cher. Le prêtre venait chercher Kernok, qui s'était enfui du lit de mort\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 2/3 [00:40<00:20, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Que mon frère écoute son ami, la langue du chef ne sera pas fourchue, il ne dira que ce qui est vrai. — Je vous écoute, chef. Je sais que vos paroles seront celles d’un sachem parlant avec un ami. — Ce matin, un peu avant le lever du soleil, je traversais avec mes guerriers la Vallée des ombres, mes guerriers suivaient l’orée de la forêt,  mais mon cœur écoutait l’ombre qui le projettait là-dedans. Mon coeur me donna des sensations de la plus grande terreur ; j’allai  voir ce que mon cœur voulait me montrer. Céadéux, le chef d’un des autres bandes de la tribu, m’attachait des yeux, comme un animal à sa maîtrise. À mesure qu’au fond de l’ombre il revenait à sa forme humaine, il vint faire un mouvement d’épouvante en voyant qu’au pied du peu de bûcher qui servait de foyer à notre village, restaient les corps de deux Hommes-noirs se tordus en spirale. Aucune flamme n’était allumée ici, à cause de la grande nuée d’orages qui avait passé ici la nuit. C’était de ces grands orages qui détruisent le feu et la mer ; l’on n’a jamais su quelle partie du ciel ils occupaient, mais comme ils poursuivirent des détonations de foudre à leurs pieds, les épais tuyaux que l’on avait installés au bas de la foyer, étaient vient se mettre à pousser de monstrueux feux. Il y avait bien des jours que mon village avait perdu ses flammes et ses vapeurs, mais jamais on n’en était resté à cette distance, jamais j’étais resté là au milieu de la nuit aussi paisible et qui paraît être si calme. Mais le temps qu’on va passer ici, ma mère, va être comme un de ces vieux temps que nous a tus raconomter mon père. D’ailleurs, je n’ai pas encore bien résolu si ce temps devra être le temps de ma jeunesse ou de ma vieillesse. L’homme que j’ai tué à mon côté, comme on l’apercevra, a les tatanes si longs qu’il semble avoir un pied plus grand que tout le reste de sa personne. Quant au']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:00<00:00, 20.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  Il brandille à Montfaucon, au bout d’une chaîne de fer, en attendant que sa carcasse déchiquetée des oiseaux tombe en la fosse du gibet, sur les ossements des camarades qui l’ont précédé. — C’est donc cela, dit Lampourde avec le plus beau sang-froid du monde, qu’on ne le voyait pas depuis quelque temps.  Ah ! lui disait-il alors, dans la perspective du gibet que le chirurgien semblait tenir de sa main et que son adjudant, dans la même perspective de la fosse la plus large à laquelle  on avait envoyé la tête, semblait avoir entendue faire plaisir le jour de la fête, le gibet que son adjudiant le poussait auprès de lui en l’espace d’une minute, ne s’était pas fait éternellement, et au-devant des yeux même de ceux qui, comme lui, n’auront jamais prit le nom de soldat.  —Le soldat, cést’à toi qui, dans l’état dans lequel tu me rencontres, ne m’apparaisse pas pour assez d’honneur, ça vaut que je n’aurois plus eu peur de tes lames et de ta voix, mon cher Lampourde ; que j’aurais eu peur de ton amour, qui me semble avoir eu pitié de toi, et qui t’avait en parlant aux miennes des derniers instans, dit-on, tué le gibet de ton père.  —Quel enfant !... ma foi !... tu dis le nom de mon père et celui d’Apostoli.  —Tes larmes vont te montrer, ne crois pas ici à l’amour... Tu ne l’as jamais aimée... tu ne l’auras jamais aimée qu’après ton divorce, lorsque je te serai venu réveiller de ce sommeil que tu as été enlevant pour t’enfermer là où tu as perdu la bonté du ciel.  —Non, mon amour est de l’amour de la fille pour son père ; comme celle de la femme par un vieillard, qui est encore celle de la jeune femme pour la marraine.  —Oui comme cela, par exemple, qui a pris la petite mère d’Apostoli pour ma soeur.  —Tu as fait de bons parolens,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "author_preds_ft, generated_texts_ft = clf_exp(model, tokenizer, texts_to_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_texts_and_labels(texts, labels, model = 'baseline', data_path = 'data'):\n",
    "    \"\"\"\n",
    "    Save the generated texts and labels in a json file\n",
    "    inputs:\n",
    "      texts: list: list of generated texts\n",
    "      labels: list: list of labels of the generated texts\n",
    "      model: str: name of the model used to generate the texts\n",
    "    \"\"\"\n",
    "    dict_text_to_author = {'text': [], 'label': []}\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "      dict_text_to_author['text'].append(texts[i])\n",
    "      #dict_text_to_author['label'].append(str(labels[i]))\n",
    "\n",
    "    with open(f\"{data_path}/{model}_generated_texts.json\", 'w+') as fd:\n",
    "      json.dump(dict_text_to_author, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver the results\n",
    "save_generated_texts_and_labels(generated_texts_ft, author_preds_ft, model = 'Mistral7B_fine_tuned', data_path = dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_texts_ft[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
